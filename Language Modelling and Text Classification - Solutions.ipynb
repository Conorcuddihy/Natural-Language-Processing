{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIZp5iLCJsNf"
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "In this lab we are going to:\n",
    "\n",
    "- Play around with text corpora <br>\n",
    "- Learn some statistics tricks in Python and NLTK <br>\n",
    "- Learn about language modelling <br>\n",
    "- Learn about n-grams <br>\n",
    "- Naive bayes as a LM <br>\n",
    "- Know about data sparsity and smoothing techniques <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGNhGL_xJsNg"
   },
   "source": [
    "# Accessing A Text Corpus\n",
    "\n",
    "Open a Python session and  obtain the <a href=\"https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/packages/corpora/brown.zip\">Brown corpus</a>, using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bmqEfcTxJsNh"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# You will need to download 'Brown' as follows:\n",
    "nltk.download('brown')\n",
    "# NLTK Downloader\n",
    "# ---------------------------------------------------------------------------\n",
    "#     d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
    "# ---------------------------------------------------------------------------\n",
    "# Downloader> d\n",
    "#\n",
    "# Download which package (l=list; x=cancel)?\n",
    "#   Identifier> brown\n",
    "#     Downloading package brown to /home/jmccrae/nltk_data...\n",
    "#       Unzipping corpora/brown.zip.\n",
    "\n",
    "# The following should now work:\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# read a list of the words in the Brown Corpus\n",
    "list_words = brown.words()\n",
    "\n",
    "print(list_words[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dd1pPOzOJsNl"
   },
   "source": [
    "We can access the corpus as a list of words, or a list of sentences (where each sentence is itself just a list of words). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aMW1n29KJsNl"
   },
   "outputs": [],
   "source": [
    "brown.sents()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xhK6xvzZJsNo"
   },
   "source": [
    "The Brown corpus consists of different categories. We can list the available categories as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2vgrRreJsNp"
   },
   "outputs": [],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6nxFYq7iJsNs"
   },
   "source": [
    "We can access the text of a certain category as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pmMl7AVlJsNt"
   },
   "outputs": [],
   "source": [
    "brown.words(categories='fiction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IPZGc_taJsNx"
   },
   "source": [
    "**Exercise 1: **\n",
    "\n",
    "What is the frequency of the word (ignoring case) &lsquo;world&rsquo; in the news category in Brown corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SWAJ0nWJsNy"
   },
   "outputs": [],
   "source": [
    "def count_freq(category, given_word):\n",
    "    count = 0\n",
    "    for word in brown.words(categories=category):\n",
    "        word = word.lower()\n",
    "        if word == given_word:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "print(count_freq('news', 'world'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39GUO6k7JsN2"
   },
   "source": [
    "# Frequency of Words\n",
    "\n",
    "We can easily get the frequency distribution of the words in a corpus as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lc6-L4u5JsN3"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "news_text = brown.words(categories='news')\n",
    "fdtest = FreqDist(list_words)\n",
    "# the frequency of each vocabulary item in the text\n",
    "fd = FreqDist(news_text)\n",
    "\n",
    "# total number of samples\n",
    "print (fd.N()) \n",
    "\n",
    "# how many unique words does this corpus have\n",
    "print (fd.B())\n",
    "\n",
    "# Get a list of the top 10 words sorted by frequency\n",
    "print(fd.most_common(10))\n",
    "print (len(list_words))\n",
    "print (fdtest.N())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j68-epFbJsN9"
   },
   "source": [
    "**Exercise 2:**\n",
    "In the Brown Corpus, in which category(s) of the  news, government and editorial categories, the word (ignoring case) &lsquo;world&rsquo; has the highest total frequency?\n",
    "* news\n",
    "* government\n",
    "* editorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u7xQiOybJsN-"
   },
   "outputs": [],
   "source": [
    "print(count_freq('news', 'world'))\n",
    "print(count_freq('government', 'world'))\n",
    "print(count_freq('editorial', 'world'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0E0wMa_lJsOC"
   },
   "source": [
    "# Probabilities\n",
    "\n",
    "**Exercise 3:**\n",
    "Calculate probabilities (relative frequency) of all words for only __news__ category in Brown corpora.\n",
    "What is the probability of the words &lsquo;jury&rsquo; and &lsquo;government&rsquo;?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fIajkqTJsOD"
   },
   "outputs": [],
   "source": [
    "def prob(category, given_word):\n",
    "    count = 0\n",
    "    total = 0\n",
    "\n",
    "    for word in brown.words(categories=category):\n",
    "        word = word.lower()\n",
    "        if word == given_word:\n",
    "            count += 1\n",
    "        total += 1\n",
    "            \n",
    "    return float(count)/total\n",
    "\n",
    "print(prob('news','jury'))\n",
    "print(prob('news','government'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ti50pxuhJsOJ"
   },
   "source": [
    "# N-Grams\n",
    "\n",
    "The Probabilisic Language Models (a.k.a n-gram LM) is developed to construct the joint probability distribution of a sequence of words. Based on the Markov assumption, the process of predicting a word sequence is broken up into predicting one word at a time.\n",
    "\n",
    "We can extract unigrams, and bigrams from a corpus as follows:\n",
    "In this example, we are going to generate unigrams and bigrams from the novel Emma by Jane Austen from The Gutenberg Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Axi0TG0SJsOJ"
   },
   "outputs": [],
   "source": [
    "#explore the corpus\n",
    "nltk.corpus.gutenberg.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9mGZEMHRJsOO"
   },
   "outputs": [],
   "source": [
    "# get the text of the novel Emma by Jane Austen \n",
    "emma_words = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "emma = \" \".join(emma_words) \n",
    "emma[:500] #first 500 words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POUxtP6SJsOR"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = nltk.word_tokenize(emma)\n",
    "tokens[:20] #first 20 token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tGWYyPdAJsOU"
   },
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "#unigrams\n",
    "print (list(ngrams(word_tokenize(emma), 1))[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JGxNg9QZJsOW"
   },
   "outputs": [],
   "source": [
    "#bigrams\n",
    "print (list(ngrams(word_tokenize(emma), 2)))\n",
    "\n",
    "#or simply\n",
    "print(list(nltk.bigrams(emma_words))[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63c7PgwrJsOZ"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "\n",
    "#Make a conditional frequency distribution of all the bigrams in the novel Emma by Jane Austen from The Gutenberg Corpus\n",
    "bigrams = nltk.bigrams(emma_words)\n",
    "\n",
    "cfd = ConditionalFreqDist(bigrams)\n",
    "\n",
    "#get the most frequently used word after ‘fully’\n",
    "cfd['fully']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X0jebwfGJsOc"
   },
   "outputs": [],
   "source": [
    "#same with 'good' but sort by freq\n",
    "cfd['good'].most_common(20) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ke4_uXkvJsOe"
   },
   "source": [
    "**Exercise 4:**\n",
    "Write a function to find the most common phrases (trigrams) in the __fiction__ category of the brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DaNKUaVxJsOe"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "fiction_text = brown.words(categories='fiction')\n",
    "trigram =  [t for t in nltk.trigrams(fiction_text)]\n",
    "freq = nltk.FreqDist(trigram) #have you noticed the difference between ConditionalFreqDist and FreqDist!\n",
    "freq.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6rWK4t9GJsOi"
   },
   "source": [
    "# Probabilistic modeling\n",
    "\n",
    "\n",
    "## Naïve Bayes\tas\ta\tLanguage\tModel\n",
    "Based on probabilities of words in only the news and fiction categories in the brown corpus, classify the phrase 'mysterious murder case' to one of these categories. \n",
    "\n",
    "You should implement Naive Bayes classifier using probabilities of each word:\n",
    "\n",
    "$P(fiction|mysterious\\ murder\\ case) \\propto P(mysterious|fiction) \\times P(murder|fiction) \\times P(case|fiction) \\times P(fiction)$\n",
    "where $P(news) = 0.5$ and $P(fiction) = 0.5$\n",
    "\n",
    "**Exercise 5:**\n",
    "Write a general purpose Naive Bayes classifier such as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RU4kDNJXJsOk"
   },
   "outputs": [],
   "source": [
    "from random import random\n",
    "\n",
    "def calculate_probability(phrase, category):\n",
    "    p = 1.0\n",
    "    for word in phrase.split():\n",
    "        word = word.lower()\n",
    "        p *= prob(category, word)\n",
    "        return p * 0.5\n",
    "\n",
    "def naive_bayes(phrase):\n",
    "    news_prob = calculate_probability(phrase, 'news')\n",
    "    fiction_prob = calculate_probability(phrase, 'fiction')\n",
    "    if news_prob > fiction_prob:\n",
    "        return 0 #news\n",
    "    else:\n",
    "        return 1 #fiction\n",
    "\n",
    "print(naive_bayes(\"mysterious murder case\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xq9JLzlOJsOo"
   },
   "source": [
    "## Smoothing\n",
    "\n",
    "A simple n-gram model would give zero probability to all of the combination that were not encountered in the training corpus, i.e. it would most likely give zero probability to most of the out-of-sample test cases. This problem is known as data sparsity and the traditional solution to it is to use smoothing techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HkmHcqj2JsOo"
   },
   "source": [
    "### Example: bigram model\n",
    "\n",
    "(pen and paper exercises)\n",
    "\n",
    "Given Corpus:\n",
    "\n",
    "$JOHN\\ READ\\ MOBY\\ DICK$\n",
    "<br>\n",
    "$MARY\\ READ\\ A\\ DIFFERENT\\ BOOK$\n",
    "<br>\n",
    "$SHE\\ READ\\ A\\ BOOK\\ BY\\ CHER$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DRUn85AHJsOo"
   },
   "source": [
    "**Exercise 6:**\n",
    "Calculate the probability of the sentence \"JOHN READ A BOOK\"?\n",
    "![Solution Exercise 6](https://preview.ibb.co/igWFpK/exercises6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_k_g-EAyJsOp"
   },
   "source": [
    "**Exercise 7:**\n",
    "What is the $p(CHER\\ READ\\ A\\ BOOK)$?\n",
    "\n",
    "![Solution Exercise 7](https://preview.ibb.co/gcHYbz/exercises7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "te21TeaNJsOq"
   },
   "source": [
    "### Add-one smoothing\n",
    "\n",
    "$p(w_i|w_{i-1}) = \\frac{1 + c(w_{i−1} w_i)} {\\sum{w_i} [1 + c(w_{i−1} w_i)] }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-lzNDOr2JsOq"
   },
   "source": [
    "**Exercise 8:**\n",
    "Re-calculate the $p(JOHN\\ READ\\ A\\ BOOK)$ and $p(CHER\\ READ\\ A\\ BOOK)$ using add-one smoothing\n",
    "\n",
    "![Solution Exercise 8](https://preview.ibb.co/bYhc3e/exercises8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E6ge7bBuJsOr"
   },
   "source": [
    "### Other Smoothing methods\n",
    "- Additive smoothing\n",
    "- Good-Turing estimate\n",
    "- Jelinek-Mercer smoothing (interpolation)\n",
    "- Katz smoothing (backoff)\n",
    "- Witten-Bell smoothing\n",
    "- Absolute discounting\n",
    "- Kneser-Ney smoothing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab 3 - Language Modelling and Text Classification - Solutions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
