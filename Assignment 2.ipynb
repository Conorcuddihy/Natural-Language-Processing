{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sC-LZ20S_WUr"
   },
   "source": [
    "# Assignment 2: Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xqCFJBv_WUt"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iq8sr9x17KhU"
   },
   "source": [
    "## Task 1: Named Entity Annotation (10 Marks)\n",
    "\n",
    "Using the IOB tagging scheme annotate all of the named entities (PERson, LOCation, ORGanisation, TIME) in the following sentence:\n",
    "\n",
    "*Wayne Rooney is a professional footballer from England who last played for Major League Soccer club D.C. United and will join Derby County in January 2020.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htlSW1ad81D-"
   },
   "source": [
    "Edit this cell and write your annotation below the line. (Note that you don't have to write code for this task, you have to annotate it manually)\n",
    "\n",
    "---\n",
    "\n",
    "**B_PER**:Wayne \n",
    "**I_PER**:Rooney \n",
    "**O**:is \n",
    "**O**:a \n",
    "**O**:professional \n",
    "**O**:footballer \n",
    "**O** from \n",
    "**B_LOC**:England \n",
    "**O**:who \n",
    "**O**:last \n",
    "**O**:played \n",
    "**O**:for \n",
    "**B_ORG**:Major \n",
    "**I_ORG**:League \n",
    "**I_ORG**:Soccer \n",
    "**O**:club \n",
    "**B_ORG**:D.C. \n",
    "**I_ORG**:United \n",
    "**O**:and \n",
    "**O**:will \n",
    "**O**:join \n",
    "**B_ORG**:Derby \n",
    "**I_ORG**:County \n",
    "**O**:in \n",
    "**B_TIME**:January \n",
    "**I_TIME**:2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZNmDWxj-V-J"
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "### For subsequent tasks in this assignment, you will work with the documents in `football_players.txt` to perform various information extraction tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V9YE4n6u7olU",
    "outputId": "bf1813fe-6992-4741-9496-56e526365db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 24172  100 24172    0     0  33021      0 --:--:-- --:--:-- --:--:-- 32976\n"
     ]
    }
   ],
   "source": [
    "# Download the text file (uncomment the line below in this cell, if not already downloaded from Blackboard)\n",
    "!curl \"https://ideone.com/plain/OvwDXZ\" > football_players.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpSaij2b73Vj"
   },
   "source": [
    " Read all the documents from `football_players.txt` into a list called `docs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qteh89cs7q4x"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "with open('football_players.txt', encoding='utf-8') as f:\n",
    "  docs = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCEJrJ-p_WU1"
   },
   "source": [
    "## Task 2 (10 Marks)\n",
    "Write a function that takes a document and returns a list of sentences with part-of-speech tags.\n",
    "\n",
    "Please keep in mind that the expected output is a list within a list as shown below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO7Fyfq7DYxW"
   },
   "source": [
    "Hint: For this task you need to perform three steps:\n",
    "1. Sentence Segmentation\n",
    "1. Word Tokenization\n",
    "1. Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U3MCJIcR_WU2"
   },
   "outputs": [],
   "source": [
    "def ie_preprocess(document):\n",
    "  sentences = nltk.sent_tokenize(document) \n",
    "  words = [nltk.word_tokenize(sent) for sent in sentences] \n",
    "  tagged_sentences = [nltk.pos_tag(word) for word in words]\n",
    "\n",
    "  return tagged_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-E04CUNb_WU6"
   },
   "source": [
    "Run the cell below to verify your result for the second sentence in the first document.\n",
    "Expected output: \n",
    "`[('He', 'PRP'), ('is', 'VBZ'), ('a', 'DT'), ('forward', 'NN'), ('and', 'CC'), ('serves', 'NNS'), ('as', 'IN'), ('captain', 'NN'), ('for', 'IN'), ('Portugal', 'NNP'), ('.', '.')]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R30taRgf_WU6",
    "outputId": "d7f387bf-0540-40ce-933f-ba27fd1baabc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('He', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('forward', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('serves', 'NNS'),\n",
       " ('as', 'IN'),\n",
       " ('captain', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('Portugal', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_doc = docs[0]\n",
    "tagged_sentences = ie_preprocess(first_doc)\n",
    "tagged_sentences[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tYTwrZId_WU_"
   },
   "source": [
    "## Task 3 (20 Marks)\n",
    "Write a function that takes a list of tokens with POS tags for a sentence and returns a list of named entities (NE). \n",
    "\n",
    "Hint: Use `binary = True` while calling NE chunk function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fC0iqJJ_WU_"
   },
   "outputs": [],
   "source": [
    "def find_named_entities(sent):\n",
    "  named_entities = []\n",
    "\n",
    "  tree = nltk.ne_chunk(sent,binary=True)\n",
    "\n",
    "  for subtree in tree.subtrees():\n",
    "    if subtree.label() == 'NE':\n",
    "        entity = \"\"\n",
    "        for leaf in subtree.leaves():\n",
    "            entity = entity + leaf[0] + \" \"\n",
    "        named_entities.append(entity.strip())\n",
    "\n",
    "  return named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Td5yJ8cgFScx"
   },
   "source": [
    "Run the cell below to verify your result for the first sentence in the first document.\n",
    "Expected output: `['Cristiano Ronaldo', 'Santos Aveiro', 'ComM', 'GOIH', 'Portuguese', 'Portuguese', 'Spanish', 'Real Madrid', 'Portugal']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FijjdAPWFsp2",
    "outputId": "44415adf-c885-446e-e65c-f9243318a950"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cristiano Ronaldo',\n",
       " 'Santos Aveiro',\n",
       " 'ComM',\n",
       " 'GOIH',\n",
       " 'Portuguese',\n",
       " 'Portuguese',\n",
       " 'Spanish',\n",
       " 'Real Madrid',\n",
       " 'Portugal']"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences = ie_preprocess(docs[0])\n",
    "find_named_entities(tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XHMp7xtK_WVE"
   },
   "source": [
    "## Task 4 (5 Marks)\n",
    "\n",
    "Implement the `find_all_named_entities` function below to find **all** NEs in a given document.\n",
    "\n",
    "Hint: Use `find_named_entities` implemented above for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwFlzQx4_WVF"
   },
   "outputs": [],
   "source": [
    "def find_all_named_entities(doc):\n",
    "  named_entities = []\n",
    "\n",
    "  tagged_sentences = ie_preprocess(doc)\n",
    "\n",
    "  for sent in tagged_sentences:\n",
    "    named_entities.extend(find_named_entities(sent))\n",
    "  \n",
    "  return named_entities   # return a flat list and not a list of lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bdM0-LZlJy4u"
   },
   "source": [
    "How many named entities did you find in the first document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ajmnnOqJ8V6",
    "outputId": "ad90b91f-9d58-4d49-ccbf-fda9a0136781"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities = find_all_named_entities(docs[0])\n",
    "len(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x2AzD9MVNIx2"
   },
   "source": [
    "## Task 5 (5 Marks)\n",
    "\n",
    "Find named entities across **all** documents in `football_players.txt`, and save the result into a single flat list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YULMcK1-NSR9"
   },
   "outputs": [],
   "source": [
    "all_named_entities = []\n",
    "\n",
    "for doc in docs:\n",
    "  named_entities = find_all_named_entities(doc)\n",
    "  all_named_entities.extend(named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaM9Cs9zNGM2"
   },
   "source": [
    "How many named entities did you find across all documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCNIrC_SNpHQ",
    "outputId": "16932024-b662-499c-bcad-b2e5c5156d59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_named_entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7-ma9lJ_WVJ"
   },
   "source": [
    "## Task 6 (40 Marks)\n",
    "\n",
    "Write functions to extract the name of the player, country of origin and date of birth as well as the following relations: team(s) of the player and position(s) of the player.\n",
    "\n",
    "Hint: Use the `re.compile()` function to create the extraction patterns.\n",
    "\n",
    "Reference: https://docs.python.org/3/howto/regex.html\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0TUQTnkvlJfB"
   },
   "source": [
    "1. **`name_of_the_player`** <br>\n",
    "    - **Named Entity Recognition**: Extract the first named entity or the most frequently occuring named entity with the label `PERSON`. An issue highlighted by many students here is that some names are matched only partially following this approach. This is because the `NER` module in `nltk` is trained on English data while there are some Spanish and Portugese names in the football corpus which can lead to some names being missed out.\n",
    "    - **Noun Phrase Extraction**: A similar approach is to parse the documents using grammar rules and extract `NNP` (noun phrases) using the POS-tags.\n",
    "    - **Pattern matching using Regex**: The name of the player is the first thing mentioned each document, so extracting this pattern using a regular expression is the easiest way to retrieve the name of the player.\n",
    "1. **`country_of_origin`** <br>\n",
    "    - **Named Entity Recognition**: Extract the first named entity or the most frequently occuring named entity with the label `GPE`.\n",
    "    - **Relation Extraction**: <br>\n",
    "        - Each document contains the information about the *national team* of the player and this relation can be used here.\n",
    "        - Another useful relation is \"*`X` is a `Y` professional footballer*\" where `X` is the name of the player and `Y` is the nationality of the player. However, this does not find the name of the country, rather it finds the nationaity, we have penalised solutions here which do not return the name of the country. In some solutions this fixed by either using a custom mapping or using wordnet synsets or using edit distance to convert from nationality to country name.\n",
    "        - An assumption here correctly highlighted by few students is that the information about nationality or national team is essentially a proxy because the country of origin can be something different.\n",
    "1. **`date_of_birth`** <br>\n",
    "    - **Relation Extraction**: Extract the relation \"*`X` born `Y`*\" where `X` is the name of the player and `Y` is the date of birth. The can be implemented using a regular expression where the pattern for the date looks something like this: `(\\d+ \\w+ \\d{4})`.\n",
    "    - Instead of using regex, some solutions use grammar rules like `<CD><NNP><CD>` to capture the pattern for the date which is also a valid solution here.\n",
    "    - Some solutions were penalised here as they only searched for the date pattern without specifying the `born` relation and returned multiple dates.\n",
    "1. **`team_of_the_player`**<br>\n",
    "    - **Relation Extraction**: Extract the relation \"*`X` plays for `Y`*\" or the relation \"*`X` played for `Y`*\" where `X` is the player and `Y` contains the names of teams. This requires some post-processing to correctly identify the names of the teams which can be done with named entity recognition by looking for `ORG` and `GPE` labels or by extracting terms and phrases using grammar rules. This is sufficient for the most part and can be improved further by extracting similar relations like \"*`X` signed for `Y`*\" or \"*`X` transferred to `Y`*\" or \"*`X` joined `Y`*\"\n",
    "    - Other solutions extract this relation using a combination of `club`, `FC` and `national team` patterns which works fine too.\n",
    "    - Some solutions searched the names of teams inside the documents using a pre-defined list of teams. This is a somewhat naive solution since it can be difficult to know ahead of time the names of all the teams as the information in the documents spans a large variety of countries and soccer leagues.\n",
    "    - For almost all the players, the documents contain detailed information about multiple football clubs they have played as well as information about the national team. So solutions which omit either the national team or the club were penalised here.\n",
    "1. **`position_of_the_player`**<br>\n",
    "    - **Relation Extraction**: Extract the relation \"*`X` plays as a `Y`*\" or the relation \"*`X` is a `Y`*\" where `X` is the player and `Y` is the position. \n",
    "    - Similar to country and teams above, term extraction on noun phrase extraction can also be used here to identify the name of the positions.\n",
    "    - Along with the relations mentioned above, some solutions searched for positions inside the documents using a pre-defined list of football positions. Unlike teams, this is acceptable here since there is only a small number of positions on a football field which have consistent and fixed names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2 - Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
